{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 영어 토크나이징"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. nltk 토크나이징\n",
    "\n",
    "https://datascienceschool.net/view-notebook/118731eec74b4ad3bdd2f89bab077e1b/\n",
    "\n",
    "■ 설치하기\n",
    "\n",
    "1. conda install nltk\n",
    "2. import nltk -> nltk.download() -> all-corpora (이외는 ...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Often', 'the', 'hardest', 'part', 'of', 'solving', 'a', 'machine', 'learning', 'problem', 'can', 'be', 'finding', 'the', 'right', 'estimator', 'for', 'the', 'job']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentence = \"Often the hardest part of solving a machine learning problem can be finding the right estimator for the job\"\n",
    "\n",
    "print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Often the hardest part of solving a machine learning problem can be finding the right estimator for the job.Different estimators are better suited for different types of data and different problems.The flowchart below is designed to give users a bit of a rough guide on how to approach problems with regard to which estimators to try on your data.Click on any estimator in the chart below to see its documentation.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "paragraph = \"Often the hardest part of solving a machine learning problem can be finding the right estimator for the job.Different estimators are better suited for different types of data and different problems.The flowchart below is designed to give users a bit of a rough guide on how to approach problems with regard to which estimators to try on your data.Click on any estimator in the chart below to see its documentation.\"\n",
    "\n",
    "print(sent_tokenize(paragraph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.spacy 토크나이징"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "# nlp = spacy.load('en')   # en 객체 생성  -> error\n",
    "nlp = en_core_web_sm.load()   # en 대신 사용가능\n",
    "\n",
    "sentence = \"Often the hardest part of solving a machine learning problem can be finding the right estimator for the job.Different estimators are better suited for different types of data and different problems.The flowchart below is designed to give users a bit of a rough guide on how to approach problems with regard to which estimators to try on your data.Click on any estimator in the chart below to see its documentation.\"\n",
    "\n",
    "doc = nlp(sentence)  # 분석객체 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Often', 'the', 'hardest', 'part', 'of', 'solving', 'a', 'machine', 'learning', 'problem', 'can', 'be', 'finding', 'the', 'right', 'estimator', 'for', 'the', 'job', '.', 'Different', 'estimators', 'are', 'better', 'suited', 'for', 'different', 'types', 'of', 'data', 'and', 'different', 'problems', '.', 'The', 'flowchart', 'below', 'is', 'designed', 'to', 'give', 'users', 'a', 'bit', 'of', 'a', 'rough', 'guide', 'on', 'how', 'to', 'approach', 'problems', 'with', 'regard', 'to', 'which', 'estimators', 'to', 'try', 'on', 'your', 'data', '.', 'Click', 'on', 'any', 'estimator', 'in', 'the', 'chart', 'below', 'to', 'see', 'its', 'documentation', '.']\n",
      "['Often the hardest part of solving a machine learning problem can be finding the right estimator for the job.', 'Different estimators are better suited for different types of data and different problems.', 'The flowchart below is designed to give users a bit of a rough guide on how to approach problems with regard to which estimators to try on your data.', 'Click on any estimator in the chart below to see its documentation.']\n"
     ]
    }
   ],
   "source": [
    "word_tokenized_sentence = [token.text for token in doc]\n",
    "sentence_tokenized_list = [sent.text for sent in doc.sents]\n",
    "\n",
    "print(word_tokenized_sentence)\n",
    "print(sentence_tokenized_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [(token.text, token.pos) for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한글 토크나이징\n",
    "\n",
    "## KoNLPy\n",
    "\n",
    "https://datascienceschool.net/view-notebook/70ce46db4ced4a999c6ec349df0f4eb0/\n",
    "\n",
    "https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/05/10/postag/\n",
    "\n",
    "■ 형태소 분석기 목록\n",
    "\n",
    "- Hannanum\n",
    "- Kkma  # 정확한 테깅\n",
    "- Komoran   # 중간정도 ( 빈 줄이 있으면 에러 남 )\n",
    "- Mecab  # 제일 빠르고 정확 -> 일본에서 만듬\n",
    "- Okt(Twitter)   # 속도\n",
    "\n",
    "■ 한글 데이터 목록\n",
    "\n",
    "- kolaw : 한국 법률 말뭉치\n",
    "- kobill : 국회 의안 말뭉치\n",
    "\n",
    "■ 설치하기\n",
    "\n",
    "1. java -version 확인 : 1.8이상 (jdk)\n",
    "2. JAVA_HOME: jdk 파일 위치 환경설정\n",
    "3. JPype1-0.6.3-cp37-cp37m-win_amd64.whl # 파일 다운로드 (python 3.7 버전)\n",
    "4. (다운 받은 파일 위치에서) pip install JPype1-0.6.3-cp37-cp37m-win_amd64.whl\n",
    "5. pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'\", '텐서', '플로', '와', '머신', '러닝', '으로', '시작', '하는', '자연어', '처리', \"'\", '책', '완독', '을', '목표', '로', '한다', '.']\n",
      "[\"'\", '텐서', '플로', '와', '머신', '러닝', '으로', '시작', '하다', '자연어', '처리', \"'\", '책', '완독', '을', '목표', '로', '하다', '.']\n",
      "['텐서', '플로', '머신', '러닝', '시작', '자연어', '처리', '책', '완독', '목표']\n",
      "['텐서플로', '텐서플로와 머신러닝', '시작', '시작하는 자연어처리', '책 완독', '목표', '텐서', '플로', '머신', '러닝', '자연어', '처리', '완독']\n",
      "[(\"'\", 'Punctuation'), ('텐서', 'Noun'), ('플로', 'Noun'), ('와', 'Josa'), ('머신', 'Noun'), ('러닝', 'Noun'), ('으로', 'Josa'), ('시작', 'Noun'), ('하는', 'Verb'), ('자연어', 'Noun'), ('처리', 'Noun'), (\"'\", 'Punctuation'), ('책', 'Noun'), ('완독', 'Noun'), ('을', 'Josa'), ('목표', 'Noun'), ('로', 'Josa'), ('한다', 'Verb'), ('.', 'Punctuation')]\n",
      "[\"'/Punctuation\", '텐서/Noun', '플로/Noun', '와/Josa', '머신/Noun', '러닝/Noun', '으로/Josa', '시작/Noun', '하는/Verb', '자연어/Noun', '처리/Noun', \"'/Punctuation\", '책/Noun', '완독/Noun', '을/Josa', '목표/Noun', '로/Josa', '한다/Verb', './Punctuation']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt #(Twitter)\n",
    "\n",
    "okt = Okt()  # 객체 생성만 다름\n",
    "\n",
    "text = \"'텐서플로와 머신러닝으로 시작하는 자연어처리' 책 완독을 목표로 한다.\"\n",
    "\n",
    "print(okt.morphs(text))   # 형태소 단위로 나눔\n",
    "print(okt.morphs(text, stem=True))   # 형태소 단위로 나눔 -> 어간추출\n",
    "\n",
    "print(okt.nouns(text))    # 명사\n",
    "print(okt.phrases(text))    # 어절\n",
    "\n",
    "print(okt.pos(text))    # 품사 태깅\n",
    "print(okt.pos(text, join=True))    # 품사 태깅 -> 형태소와 품사를 붙여서 리스트 화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토크나이저(띄어쓰기) -> 인베딩\n",
    "\n",
    "Q. 단, 토크나이징에서 형태소 분석을 하는 이유 ? \n",
    "\n",
    "1. 같은 단어: 품사(명사, 동사)를 나눠서 따로 인베딩 가능\n",
    "2. 먹다, 먹었다: 어간을 합쳐서 따로 인베딩 가능"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
