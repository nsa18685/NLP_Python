{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.3 Linear Regression Example with Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "idea : word to vec\n",
    "\n",
    "- string으로 다른 단어와 유사도를 비교를 하는 것이 아닌 의미적으로 다른 단어와 비교를 하는 것!\n",
    "\n",
    "- 벡터화된 단어간의 유사도: jaccard, cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Feature Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_IN_PATH = './data_in/'\n",
    "DATA_OUT_PATH = './data_out/'\n",
    "TRAIN_CLEAN_DATA = 'train_clean.csv'\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "TEST_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(DATA_IN_PATH + TRAIN_CLEAN_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = list(train_data['review'])\n",
    "sentiments = list(train_data['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### word2vec을 활용한 모델 구현\n",
    "\n",
    ": word2vec의 경우 단어로 표현된 리스트를 입력값으로 넣어야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []  # 단어집합\n",
    "for review in reviews:\n",
    "    sentences.append(review.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### word2vec 벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습에 필요한 하이퍼 파라미터\n",
    "num_features = 300   # 임베딩된 벡터의 차원을 정한다, 워드 벡터 특징값 수 \n",
    "min_word_count = 40  # 모델에 의미 있는 단어를 가지고 학습하기 위해 적은 빈도 수의 단어들은 학습하지 않는다, 단어에 대한 최소 빈도 수\n",
    "num_workers = 4      # 모델 학습시 필요한 프로세스 개수를 지정한다.\n",
    "context = 10         # 컨텍스트 윈도우 크기를 지정한다. (주변에 있는 단어를 관여, 양옆으로 10개씩)\n",
    "downsampling = 1e-3  # 라벨에 대한 다운샘플링 비율을 지정한다. (전체 단어의 0.001을 어떤 단어가 넘는 순간, 빈번하게 등장한다고 판단하고 이 단어에 대해 다운샘플링을 진행하는 것으로 보임, 계산속도를 위한 feature 추상화?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging    # 로그 메시지를 양식에 맞게 info 수준으로 보기(학습과정)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-08 21:46:24,374 : INFO : 'pattern' package not found; tag filters are not available for English\n",
      "2019-04-08 21:46:24,383 : INFO : collecting all words and their counts\n",
      "2019-04-08 21:46:24,385 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-04-08 21:46:24,887 : INFO : PROGRESS: at sentence #10000, processed 1205223 words, keeping 51374 word types\n",
      "2019-04-08 21:46:25,429 : INFO : PROGRESS: at sentence #20000, processed 2396605 words, keeping 67660 word types\n",
      "2019-04-08 21:46:25,764 : INFO : collected 74065 word types from a corpus of 2988089 raw words and 25000 sentences\n",
      "2019-04-08 21:46:25,766 : INFO : Loading a fresh vocabulary\n",
      "2019-04-08 21:46:25,863 : INFO : effective_min_count=40 retains 8160 unique words (11% of original 74065, drops 65905)\n",
      "2019-04-08 21:46:25,865 : INFO : effective_min_count=40 leaves 2627273 word corpus (87% of original 2988089, drops 360816)\n",
      "2019-04-08 21:46:25,908 : INFO : deleting the raw counts dictionary of 74065 items\n",
      "2019-04-08 21:46:25,911 : INFO : sample=0.001 downsamples 30 most-common words\n",
      "2019-04-08 21:46:25,913 : INFO : downsampling leaves estimated 2494384 word corpus (94.9% of prior 2627273)\n",
      "2019-04-08 21:46:25,956 : INFO : estimated required memory for 8160 words and 300 dimensions: 23664000 bytes\n",
      "2019-04-08 21:46:25,959 : INFO : resetting layer weights\n",
      "2019-04-08 21:46:26,174 : INFO : training model with 4 workers on 8160 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-04-08 21:46:27,193 : INFO : EPOCH 1 - PROGRESS: at 16.30% examples, 411135 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-08 21:46:28,207 : INFO : EPOCH 1 - PROGRESS: at 33.15% examples, 413639 words/s, in_qsize 6, out_qsize 1\n",
      "2019-04-08 21:46:29,220 : INFO : EPOCH 1 - PROGRESS: at 52.03% examples, 431419 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-08 21:46:30,232 : INFO : EPOCH 1 - PROGRESS: at 69.63% examples, 432152 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-08 21:46:31,237 : INFO : EPOCH 1 - PROGRESS: at 86.86% examples, 431447 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-08 21:46:31,959 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-08 21:46:31,976 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-08 21:46:31,987 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-08 21:46:32,000 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-08 21:46:32,002 : INFO : EPOCH - 1 : training on 2988089 raw words (2494549 effective words) took 5.8s, 429121 effective words/s\n",
      "2019-04-08 21:46:33,015 : INFO : EPOCH 2 - PROGRESS: at 16.30% examples, 410685 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-08 21:46:34,023 : INFO : EPOCH 2 - PROGRESS: at 32.82% examples, 411048 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-08 21:46:35,039 : INFO : EPOCH 2 - PROGRESS: at 49.68% examples, 412588 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-08 21:46:36,050 : INFO : EPOCH 2 - PROGRESS: at 66.06% examples, 409961 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-08 21:46:37,059 : INFO : EPOCH 2 - PROGRESS: at 83.97% examples, 416742 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-08 21:46:37,996 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-08 21:46:38,008 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-08 21:46:38,010 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-08 21:46:38,023 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-08 21:46:38,024 : INFO : EPOCH - 2 : training on 2988089 raw words (2494558 effective words) took 6.0s, 414851 effective words/s\n",
      "2019-04-08 21:46:39,035 : INFO : EPOCH 3 - PROGRESS: at 14.36% examples, 362205 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-08 21:46:40,051 : INFO : EPOCH 3 - PROGRESS: at 29.47% examples, 368882 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-08 21:46:41,067 : INFO : EPOCH 3 - PROGRESS: at 46.11% examples, 381679 words/s, in_qsize 8, out_qsize 1\n",
      "2019-04-08 21:46:42,068 : INFO : EPOCH 3 - PROGRESS: at 62.00% examples, 385627 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-08 21:46:43,069 : INFO : EPOCH 3 - PROGRESS: at 81.75% examples, 406174 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-08 21:46:44,085 : INFO : EPOCH 3 - PROGRESS: at 98.11% examples, 404904 words/s, in_qsize 6, out_qsize 0\n",
      "2019-04-08 21:46:44,138 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-08 21:46:44,141 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-08 21:46:44,143 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-08 21:46:44,156 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-08 21:46:44,157 : INFO : EPOCH - 3 : training on 2988089 raw words (2494645 effective words) took 6.1s, 407360 effective words/s\n",
      "2019-04-08 21:46:45,168 : INFO : EPOCH 4 - PROGRESS: at 13.78% examples, 346265 words/s, in_qsize 8, out_qsize 1\n",
      "2019-04-08 21:46:46,205 : INFO : EPOCH 4 - PROGRESS: at 30.50% examples, 376985 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-08 21:46:47,219 : INFO : EPOCH 4 - PROGRESS: at 47.12% examples, 387387 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-08 21:46:48,234 : INFO : EPOCH 4 - PROGRESS: at 62.64% examples, 386549 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-08 21:46:49,269 : INFO : EPOCH 4 - PROGRESS: at 80.47% examples, 394242 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-08 21:46:50,303 : INFO : EPOCH 4 - PROGRESS: at 98.12% examples, 399143 words/s, in_qsize 6, out_qsize 0\n",
      "2019-04-08 21:46:50,344 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-08 21:46:50,355 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-08 21:46:50,371 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-08 21:46:50,384 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-08 21:46:50,386 : INFO : EPOCH - 4 : training on 2988089 raw words (2494429 effective words) took 6.2s, 400992 effective words/s\n",
      "2019-04-08 21:46:51,401 : INFO : EPOCH 5 - PROGRESS: at 14.70% examples, 369569 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-08 21:46:52,436 : INFO : EPOCH 5 - PROGRESS: at 31.20% examples, 385190 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-08 21:46:53,436 : INFO : EPOCH 5 - PROGRESS: at 47.48% examples, 391887 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-08 21:46:54,443 : INFO : EPOCH 5 - PROGRESS: at 62.33% examples, 386518 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-08 21:46:55,450 : INFO : EPOCH 5 - PROGRESS: at 79.04% examples, 391554 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-08 21:46:56,459 : INFO : EPOCH 5 - PROGRESS: at 95.78% examples, 394587 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-08 21:46:56,633 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-08 21:46:56,647 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-08 21:46:56,660 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-08 21:46:56,670 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-08 21:46:56,671 : INFO : EPOCH - 5 : training on 2988089 raw words (2494150 effective words) took 6.3s, 397465 effective words/s\n",
      "2019-04-08 21:46:56,672 : INFO : training on a 14940445 raw words (12472331 effective words) took 30.5s, 408978 effective words/s\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec  # 자체적으로 index생성 -> 딕셔너리(단어사전) -> 벡터\n",
    "\n",
    "# word2vec 객체를 만든다.\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "                          size=num_features, min_count = min_word_count, \\\n",
    "                          window = context, sample = downsampling) # ephochs 조절 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-08 21:49:19,730 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2019-04-08 21:49:19,732 : INFO : not storing attribute vectors_norm\n",
      "2019-04-08 21:49:19,733 : INFO : not storing attribute cum_table\n",
      "2019-04-08 21:49:20,082 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "# 모델을 저장하면 Word2Vec.load()를 통해 모델을 다시 사용할 수 있다.\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리뷰마다 단어의 개수가 모두 다르기 때문에 입력값을 하나의 형태로 만들어야 한다.\n",
    "# 문장에 있는 모든 단어의 벡터값에 대해 평균을 내서 리뷰 하나당 하나의 벡터로 만드는 방법\n",
    "# -> 단어의 정보를 살려서 붙여서 사용하는 방법을 사용하는 경우도 많음\n",
    "def get_features(words, model, num_features):\n",
    "    # 출력 벡터 초기화\n",
    "    feature_vector = np.zeros((num_features),dtype=np.float32)\n",
    "\n",
    "    num_words = 0\n",
    "    # 어휘사전 준비\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "\n",
    "    for w in words:\n",
    "        if w in index2word_set:\n",
    "            num_words += 1\n",
    "            # 사전에 해당하는 단어에 대해 단어 벡터를 더함\n",
    "            feature_vector = np.add(feature_vector, model[w]) # model: word2vec 모델\n",
    "    \n",
    "    # 문장의 단어 수만큼 나누어 단어 벡터의 평균값을 문장 벡터로 함\n",
    "    feature_vector = np.divide(feature_vector, num_words) # 임베딩하는 차원 수\n",
    "    \n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 리뷰의 평균 벡터를 구하는 함수\n",
    "def get_dataset(reviews, model, num_features):\n",
    "    dataset = list()\n",
    "\n",
    "    for s in reviews:\n",
    "        dataset.append(get_features(s, model, num_features))  # 리뷰 하나당 하나의 벡터\n",
    "\n",
    "    reviewFeatureVecs = np.stack(dataset)\n",
    "    \n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nsa18\\anaconda3\\envs\\ml_nlp\\lib\\site-packages\\ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "train_data_vecs = get_dataset(sentences, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.25949094, -0.02115187, -0.32396752, ...,  0.21128088,\n",
       "         0.18315448, -0.2859757 ],\n",
       "       [ 0.14265062,  0.06817823,  0.09425162, ...,  0.26768705,\n",
       "         0.30138168, -0.1131453 ],\n",
       "       [ 0.08122231, -0.12266844, -0.03936379, ...,  0.07450753,\n",
       "         0.07171711, -0.02665397],\n",
       "       ...,\n",
       "       [-0.28711239, -0.07738554, -0.43110138, ...,  0.22086881,\n",
       "         0.19231258, -0.15484633],\n",
       "       [-0.08024016,  0.14691758, -0.1669161 , ...,  0.23371397,\n",
       "         0.3719498 , -0.15348841],\n",
       "       [-0.32940215,  0.19108546, -0.02552948, ...,  0.09473239,\n",
       "         0.23248293, -0.36190802]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습과 검증 데이터셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "X = train_data_vecs\n",
    "y = np.array(sentiments)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SPLIT, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 선언 및 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nsa18\\anaconda3\\envs\\ml_nlp\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
       "          solver='warn', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lgs = LogisticRegression(class_weight='balanced')\n",
    "lgs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 검증 데이터셋을 이용한 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "Accuracy: 0.862800\n",
      "Precision: 0.857868\n",
      "Recall: 0.872171\n",
      "F1-Score: 0.864961\n",
      "AUC: 0.934065\n"
     ]
    }
   ],
   "source": [
    "predicted = lgs.predict(X_test)\n",
    "from sklearn import metrics\n",
    "\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test, (lgs.predict_proba(X_test)[:, 1]))\n",
    "auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "# word2vec의 경우도 항상 좋은 결과를 만들지는 않는다.\n",
    "print(\"------------\")\n",
    "print(\"Accuracy: %f\" % lgs.score(X_test, y_test))  #checking the accuracy\n",
    "print(\"Precision: %f\" % metrics.precision_score(y_test, predicted))\n",
    "print(\"Recall: %f\" % metrics.recall_score(y_test, predicted))\n",
    "print(\"F1-Score: %f\" % metrics.f1_score(y_test, predicted))\n",
    "print(\"AUC: %f\" % auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 제출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_CLEAN_DATA = 'test_clean.csv'\n",
    "\n",
    "test_data = pd.read_csv(DATA_IN_PATH + TEST_CLEAN_DATA)\n",
    "\n",
    "test_review = list(test_data['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>naturally film main themes mortality nostalgia...</td>\n",
       "      <td>\"12311_10\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>movie disaster within disaster film full great...</td>\n",
       "      <td>\"8348_2\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>movie kids saw tonight child loved one point k...</td>\n",
       "      <td>\"5828_4\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>afraid dark left impression several different ...</td>\n",
       "      <td>\"7186_2\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>accurate depiction small time mob life filmed ...</td>\n",
       "      <td>\"12128_7\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review          id\n",
       "0  naturally film main themes mortality nostalgia...  \"12311_10\"\n",
       "1  movie disaster within disaster film full great...    \"8348_2\"\n",
       "2  movie kids saw tonight child loved one point k...    \"5828_4\"\n",
       "3  afraid dark left impression several different ...    \"7186_2\"\n",
       "4  accurate depiction small time mob life filmed ...   \"12128_7\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = list()\n",
    "for review in test_review:\n",
    "    test_sentences.append(review.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nsa18\\anaconda3\\envs\\ml_nlp\\lib\\site-packages\\ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "test_data_vecs = get_dataset(test_sentences, model, num_features)  # 임베딩된 벡터값을 갖게한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicted = lgs.predict(test_data_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = list(test_data['id'])\n",
    "\n",
    "answer_dataset = pd.DataFrame({'id': ids, 'sentiment': test_predicted})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(DATA_OUT_PATH):\n",
    "    os.makedirs(DATA_OUT_PATH)\n",
    "\n",
    "answer_dataset.to_csv(DATA_OUT_PATH + 'lgs_w2v_answer.csv', index=False, quoting=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
