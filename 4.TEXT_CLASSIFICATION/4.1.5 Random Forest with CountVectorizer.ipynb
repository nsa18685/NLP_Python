{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 랜덤 포레스트 분류 모델\n",
    "\n",
    ": 여러 개의 의사결정 트리의 결괏값을 평균낸 것을 결과로 사용한다.(분류, 회귀 둘다 가능)\n",
    "\n",
    "- 많은 트리를 함께 사용함으로써 정확도가 높아진다.(앙상블)\n",
    "\n",
    "\n",
    "### 2.1 의사결정 트리\n",
    "\n",
    ": 자료구조 중 하나인 트리 구조와 같은 형태로 이뤄진 알고리즘이다.\n",
    "\n",
    "- 각 노드는 하나의 질문이 된다. (스무고개와 비슷)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 패키지 및 데이터 불러오기 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer # 각 텍스트에서 횟수를 기준으로 특징추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_IN_PATH = './data_in/'\n",
    "DATA_OUT_PATH = './data_out/'\n",
    "TRAIN_CLEAN_DATA = 'train_clean.csv'\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(DATA_IN_PATH + TRAIN_CLEAN_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = list(train_data['review'])\n",
    "y = np.array(train_data['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVectorizer를 활용한 벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\", max_features = 5000) \n",
    "\n",
    "train_data_features = vectorizer.fit_transform(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<25000x5000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1975048 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2496)\t1\n",
      "  (0, 2122)\t1\n",
      "  (0, 4273)\t1\n",
      "  (0, 1578)\t1\n",
      "  (0, 1399)\t1\n",
      "  (0, 1589)\t1\n",
      "  (0, 1290)\t1\n",
      "  (0, 801)\t1\n",
      "  (0, 386)\t1\n",
      "  (0, 1213)\t1\n",
      "  (0, 4279)\t1\n",
      "  (0, 1849)\t1\n",
      "  (0, 283)\t1\n",
      "  (0, 4861)\t2\n",
      "  (0, 3280)\t1\n",
      "  (0, 1925)\t1\n",
      "  (0, 1503)\t1\n",
      "  (0, 4383)\t1\n",
      "  (0, 4611)\t1\n",
      "  (0, 1883)\t1\n",
      "  (0, 550)\t1\n",
      "  (0, 2322)\t1\n",
      "  (0, 1886)\t1\n",
      "  (0, 4614)\t1\n",
      "  (0, 309)\t1\n",
      "  :\t:\n",
      "  (0, 2259)\t1\n",
      "  (0, 1977)\t2\n",
      "  (0, 4878)\t1\n",
      "  (0, 2835)\t1\n",
      "  (0, 2688)\t1\n",
      "  (0, 1398)\t1\n",
      "  (0, 947)\t2\n",
      "  (0, 3556)\t2\n",
      "  (0, 4478)\t1\n",
      "  (0, 1982)\t2\n",
      "  (0, 2263)\t1\n",
      "  (0, 682)\t1\n",
      "  (0, 1873)\t1\n",
      "  (0, 4809)\t1\n",
      "  (0, 2757)\t3\n",
      "  (0, 4832)\t2\n",
      "  (0, 1273)\t1\n",
      "  (0, 3068)\t1\n",
      "  (0, 4834)\t1\n",
      "  (0, 2933)\t2\n",
      "  (0, 2590)\t1\n",
      "  (0, 4181)\t1\n",
      "  (0, 2874)\t1\n",
      "  (0, 1905)\t3\n",
      "  (0, 4267)\t1\n"
     ]
    }
   ],
   "source": [
    "print(train_data_features[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습과 검증 데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, eval_input, train_label, eval_label = train_test_split(train_data_features, y, test_size=TEST_SIZE, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 구현 및 학습 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 랜덤 포레스트 분류기에  100개 의사 결정 트리를 사용한다.\n",
    "forest = RandomForestClassifier(n_estimators = 100)   # 모형의 갯수\n",
    "\n",
    "# 단어 묶음을 벡터화한 데이터와 정답 데이터를 가지고 학습을 시작한다.\n",
    "forest.fit( train_input, train_label )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 검증 데이터셋으로 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.841600\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %f\" % forest.score(eval_input, eval_label))  # 검증함수로 정확도 측정\n",
    "# TF-IDF나 word2vec을 사용해서 입력값을 만든다면 성능이 높아질 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 제출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_CLEAN_DATA = 'test_clean.csv'\n",
    "\n",
    "test_data = pd.read_csv(DATA_IN_PATH + TEST_CLEAN_DATA)\n",
    "\n",
    "test_reviews = list(test_data['review'])\n",
    "ids = list(test_data['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_features = vectorizer.transform(test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(DATA_OUT_PATH):\n",
    "    os.makedirs(DATA_OUT_PATH)\n",
    "\n",
    "# 위에서 만든 랜덤 포레스트 분류기를 통해 예측값을 가져온다.\n",
    "result = forest.predict(test_data_features)\n",
    "\n",
    "# 판다스 데이터 프레임을 통해 데이터를 구성해서 output에 넣는다.\n",
    "output = pd.DataFrame( data={\"id\": ids, \"sentiment\": result} )\n",
    "\n",
    "# 이제 csv파일로 만든다.\n",
    "output.to_csv( DATA_OUT_PATH + \"Bag_of_Words_model.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVectorizer를 활용한 벡터화2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"char\", max_features = 5000) \n",
    "\n",
    "train_data_features = vectorizer.fit_transform(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<25000x27 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 627652 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습과 검증 데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, eval_input, train_label, eval_label = train_test_split(train_data_features, y, test_size=TEST_SIZE, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 구현 및 학습 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 랜덤 포레스트 분류기에  100개 의사 결정 트리를 사용한다.\n",
    "forest = RandomForestClassifier(n_estimators = 100) \n",
    "\n",
    "# 단어 묶음을 벡터화한 데이터와 정답 데이터를 가지고 학습을 시작한다.\n",
    "forest.fit( train_input, train_label )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 검증 데이터셋으로 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.605200\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %f\" % forest.score(eval_input, eval_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF 사용하기\n",
    "\n",
    ": 어떤 단어가 해당 문서에 자주 등장하지만 다른 문서에는 많이 없는 단어일수록 높은 값을 가짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df = 0.0, analyzer=\"word\", sublinear_tf=True, ngram_range=(0,3) max_features=5000) \n",
    "\n",
    "X = vectorizer.fit_transform(reviews)\n",
    "y = np.array(train_data['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습과 검증 데이터셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, eval_input, train_label, eval_label = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 선언 및 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 랜덤 포레스트 분류기에  100개 의사 결정 트리를 사용한다.\n",
    "forest = RandomForestClassifier(n_estimators = 100) \n",
    "\n",
    "# 단어 묶음을 벡터화한 데이터와 정답 데이터를 가지고 학습을 시작한다.\n",
    "forest.fit(train_input, train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 검증 데이터셋으로 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.847800\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %f\" % forest.score(eval_input, eval_label))  # CountVectorize & word Accuracy: 0.847600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF 사용하기2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# TF-IDF 사용하기2from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df = 0.0, analyzer=\"word\", sublinear_tf=True, max_features=5000)  # without n-gram\n",
    "\n",
    "X = vectorizer.fit_transform(reviews)\n",
    "y = np.array(train_data['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습과 검증 데이터셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, eval_input, train_label, eval_label = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 선언 및 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 랜덤 포레스트 분류기에  100개 의사 결정 트리를 사용한다.\n",
    "forest = RandomForestClassifier(n_estimators = 100) \n",
    "\n",
    "# 단어 묶음을 벡터화한 데이터와 정답 데이터를 가지고 학습을 시작한다.\n",
    "forest.fit(train_input, train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 검증 데이터셋으로 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.851200\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %f\" % forest.score(eval_input, eval_label))  # CountVectorize & word Accuracy: 0.847600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF 사용하기3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df = 0.0, analyzer=\"char\", sublinear_tf=True, ngram_range=(1,3), max_features=5000) \n",
    "\n",
    "X = vectorizer.fit_transform(reviews)\n",
    "y = np.array(train_data['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습과 검증 데이터셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, eval_input, train_label, eval_label = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 선언 및 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 랜덤 포레스트 분류기에  100개 의사 결정 트리를 사용한다.\n",
    "forest = RandomForestClassifier(n_estimators = 100) \n",
    "\n",
    "# 단어 묶음을 벡터화한 데이터와 정답 데이터를 가지고 학습을 시작한다.\n",
    "forest.fit(train_input, train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 검증 데이터셋으로 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.793800\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %f\" % forest.score(eval_input, eval_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec 사용하기\n",
    "\n",
    ": 단어의 주변을 보면 그 단어를 안다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습에 필요한 하이퍼 파라미터\n",
    "num_features = 300   # 임베딩된 벡터의 차원을 정한다, 워드 벡터 특징값 수 \n",
    "min_word_count = 40  # 모델에 의미 있는 단어를 가지고 학습하기 위해 적은 빈도 수의 단어들은 학습하지 않는다, 단어에 대한 최소 빈도 수\n",
    "num_workers = 4      # 모델 학습시 필요한 프로세스 개수를 지정한다.\n",
    "context = 10         # 컨텍스트 윈도우 크기를 지정한다. (주변에 있는 단어를 관여, 양옆으로 10개씩)\n",
    "downsampling = 1e-3  # 라벨에 대한 다운샘플링 비율을 지정한다. (전체 단어의 0.001을 어떤 단어가 넘는 순간, 빈번하게 등장한다고 판단하고 이 단어에 대해 다운샘플링을 진행하는 것으로 보임, 계산속도를 위한 feature 추상화?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging    # 로그 메시지를 양식에 맞게 info 수준으로 보기(학습과정)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for review in reviews:\n",
    "    sentences.append(review.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-08 22:35:36,470 : INFO : collecting all words and their counts\n",
      "2019-04-08 22:35:36,472 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-04-08 22:35:36,953 : INFO : PROGRESS: at sentence #10000, processed 1205223 words, keeping 51374 word types\n",
      "2019-04-08 22:35:37,416 : INFO : PROGRESS: at sentence #20000, processed 2396605 words, keeping 67660 word types\n",
      "2019-04-08 22:35:37,634 : INFO : collected 74065 word types from a corpus of 2988089 raw words and 25000 sentences\n",
      "2019-04-08 22:35:37,635 : INFO : Loading a fresh vocabulary\n",
      "2019-04-08 22:35:37,711 : INFO : effective_min_count=40 retains 8160 unique words (11% of original 74065, drops 65905)\n",
      "2019-04-08 22:35:37,712 : INFO : effective_min_count=40 leaves 2627273 word corpus (87% of original 2988089, drops 360816)\n",
      "2019-04-08 22:35:37,749 : INFO : deleting the raw counts dictionary of 74065 items\n",
      "2019-04-08 22:35:37,753 : INFO : sample=0.001 downsamples 30 most-common words\n",
      "2019-04-08 22:35:37,756 : INFO : downsampling leaves estimated 2494384 word corpus (94.9% of prior 2627273)\n",
      "2019-04-08 22:35:37,808 : INFO : estimated required memory for 8160 words and 300 dimensions: 23664000 bytes\n",
      "2019-04-08 22:35:37,810 : INFO : resetting layer weights\n",
      "2019-04-08 22:35:38,081 : INFO : training model with 4 workers on 8160 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-04-08 22:35:39,100 : INFO : EPOCH 1 - PROGRESS: at 16.94% examples, 426064 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-08 22:35:40,105 : INFO : EPOCH 1 - PROGRESS: at 33.15% examples, 414844 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-08 22:35:41,123 : INFO : EPOCH 1 - PROGRESS: at 48.74% examples, 404131 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-08 22:35:42,130 : INFO : EPOCH 1 - PROGRESS: at 64.76% examples, 401956 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-08 22:35:43,157 : INFO : EPOCH 1 - PROGRESS: at 79.04% examples, 390760 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-08 22:35:44,158 : INFO : EPOCH 1 - PROGRESS: at 94.46% examples, 389022 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-08 22:35:44,469 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-08 22:35:44,487 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-08 22:35:44,502 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-08 22:35:44,526 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-08 22:35:44,527 : INFO : EPOCH - 1 : training on 2988089 raw words (2494765 effective words) took 6.4s, 387642 effective words/s\n",
      "2019-04-08 22:35:45,545 : INFO : EPOCH 2 - PROGRESS: at 15.66% examples, 395864 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-08 22:35:46,547 : INFO : EPOCH 2 - PROGRESS: at 32.16% examples, 404478 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-08 22:35:47,567 : INFO : EPOCH 2 - PROGRESS: at 48.74% examples, 405002 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-08 22:35:48,582 : INFO : EPOCH 2 - PROGRESS: at 64.73% examples, 401967 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-08 22:35:49,615 : INFO : EPOCH 2 - PROGRESS: at 83.01% examples, 409926 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-08 22:35:50,526 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-08 22:35:50,533 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-08 22:35:50,545 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-08 22:35:50,570 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-08 22:35:50,571 : INFO : EPOCH - 2 : training on 2988089 raw words (2494587 effective words) took 6.0s, 413871 effective words/s\n",
      "2019-04-08 22:35:51,602 : INFO : EPOCH 3 - PROGRESS: at 16.65% examples, 413736 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-08 22:35:52,615 : INFO : EPOCH 3 - PROGRESS: at 32.16% examples, 398987 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-08 22:35:53,629 : INFO : EPOCH 3 - PROGRESS: at 48.39% examples, 399379 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-08 22:35:54,634 : INFO : EPOCH 3 - PROGRESS: at 65.77% examples, 406855 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-08 22:35:55,648 : INFO : EPOCH 3 - PROGRESS: at 81.46% examples, 402236 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-08 22:35:56,677 : INFO : EPOCH 3 - PROGRESS: at 97.44% examples, 399442 words/s, in_qsize 7, out_qsize 1\n",
      "2019-04-08 22:35:56,736 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-08 22:35:56,748 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-08 22:35:56,766 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-08 22:35:56,781 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-08 22:35:56,782 : INFO : EPOCH - 3 : training on 2988089 raw words (2494498 effective words) took 6.2s, 402455 effective words/s\n",
      "2019-04-08 22:35:57,805 : INFO : EPOCH 4 - PROGRESS: at 15.95% examples, 400590 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-08 22:35:58,808 : INFO : EPOCH 4 - PROGRESS: at 32.82% examples, 410737 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-08 22:35:59,815 : INFO : EPOCH 4 - PROGRESS: at 49.68% examples, 413805 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-08 22:36:00,834 : INFO : EPOCH 4 - PROGRESS: at 66.38% examples, 412120 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-08 22:36:01,849 : INFO : EPOCH 4 - PROGRESS: at 83.66% examples, 414623 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-08 22:36:02,683 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-08 22:36:02,687 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-08 22:36:02,689 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-08 22:36:02,695 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-08 22:36:02,696 : INFO : EPOCH - 4 : training on 2988089 raw words (2494474 effective words) took 5.9s, 422746 effective words/s\n",
      "2019-04-08 22:36:03,727 : INFO : EPOCH 5 - PROGRESS: at 15.95% examples, 395909 words/s, in_qsize 6, out_qsize 1\n",
      "2019-04-08 22:36:04,730 : INFO : EPOCH 5 - PROGRESS: at 33.49% examples, 416403 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-08 22:36:05,760 : INFO : EPOCH 5 - PROGRESS: at 50.00% examples, 411855 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-08 22:36:06,782 : INFO : EPOCH 5 - PROGRESS: at 67.07% examples, 412419 words/s, in_qsize 6, out_qsize 1\n",
      "2019-04-08 22:36:07,789 : INFO : EPOCH 5 - PROGRESS: at 83.34% examples, 410496 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-08 22:36:08,732 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-08 22:36:08,746 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-08 22:36:08,749 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-08 22:36:08,771 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-08 22:36:08,772 : INFO : EPOCH - 5 : training on 2988089 raw words (2494260 effective words) took 6.1s, 411196 effective words/s\n",
      "2019-04-08 22:36:08,773 : INFO : training on a 14940445 raw words (12472584 effective words) took 30.7s, 406410 effective words/s\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "# word2vec 객체를 만든다.\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "                          size=num_features, min_count = min_word_count, \\\n",
    "                          window = context, sample = downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리뷰마다 단어의 개수가 모두 다르기 때문에 입력값을 하나의 형태로 만들어야 한다.\n",
    "# 문장에 있는 모든 단어의 벡터값에 대해 평균을 내서 리뷰 하나당 하나의 벡터로 만드는 방법\n",
    "def get_features(words, model, num_features):\n",
    "    # 출력 벡터 초기화\n",
    "    feature_vector = np.zeros((num_features),dtype=np.float32)\n",
    "\n",
    "    num_words = 0\n",
    "    # 어휘사전 준비\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "\n",
    "    for w in words:\n",
    "        if w in index2word_set:\n",
    "            num_words += 1\n",
    "            # 사전에 해당하는 단어에 대해 단어 벡터를 더함\n",
    "            feature_vector = np.add(feature_vector, model[w]) # model: word2vec 모델\n",
    "    \n",
    "    # 문장의 단어 수만큼 나누어 단어 벡터의 평균값을 문장 벡터로 함\n",
    "    feature_vector = np.divide(feature_vector, num_words) # 임베딩하는 차원 수\n",
    "    \n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 리뷰의 평균 벡터를 구하는 함수\n",
    "def get_dataset(reviews, model, num_features):\n",
    "    dataset = list()\n",
    "\n",
    "    for s in reviews:\n",
    "        dataset.append(get_features(s, model, num_features))  # 리뷰 하나당 하나의 벡터\n",
    "\n",
    "    reviewFeatureVecs = np.stack(dataset)\n",
    "    \n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nsa18\\anaconda3\\envs\\ml_nlp\\lib\\site-packages\\ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "train_data_vecs = get_dataset(sentences, model, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습과 검증 데이터셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, eval_input, train_label, eval_label = train_test_split(train_data_vecs, y, test_size=TEST_SIZE, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 선언 및 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 랜덤 포레스트 분류기에  100개 의사 결정 트리를 사용한다.\n",
    "forest = RandomForestClassifier(n_estimators = 100) \n",
    "\n",
    "# 단어 묶음을 벡터화한 데이터와 정답 데이터를 가지고 학습을 시작한다.\n",
    "forest.fit(train_input, train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 검증 데이터셋으로 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.841000\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %f\" % forest.score(eval_input, eval_label)) # 항상 더 좋은 결과만을 가져오는 것은 아니다. TF-IDF & word Accuracy: 0.847800"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec 사용하기2\n",
    "\n",
    "(단어의 주변을 보면 그 단어를 안다. -> window size를 줄여보면?(5) Accuracy: 0.836800/ 늘려보면?(15) Accuracy: 0.852200)\n",
    "\n",
    "Q: word2vec에서 window_size를 정하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습에 필요한 하이퍼 파라미터\n",
    "num_features = 300   # 임베딩된 벡터의 차원을 정한다, 워드 벡터 특징값 수 \n",
    "min_word_count = 40  # 모델에 의미 있는 단어를 가지고 학습하기 위해 적은 빈도 수의 단어들은 학습하지 않는다, 단어에 대한 최소 빈도 수\n",
    "num_workers = 4      # 모델 학습시 필요한 프로세스 개수를 지정한다\n",
    "context = 15         # 컨텍스트 윈도우 크기를 지정한다 (주변 단어의 범위)\n",
    "downsampling = 1e-3  # 라벨에 대한 다운샘플링 비율을 지정한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for review in reviews:\n",
    "    sentences.append(review.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-09 20:25:00,413 : INFO : collecting all words and their counts\n",
      "2019-04-09 20:25:00,414 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-04-09 20:25:00,901 : INFO : PROGRESS: at sentence #10000, processed 1205223 words, keeping 51374 word types\n",
      "2019-04-09 20:25:01,484 : INFO : PROGRESS: at sentence #20000, processed 2396605 words, keeping 67660 word types\n",
      "2019-04-09 20:25:01,766 : INFO : collected 74065 word types from a corpus of 2988089 raw words and 25000 sentences\n",
      "2019-04-09 20:25:01,768 : INFO : Loading a fresh vocabulary\n",
      "2019-04-09 20:25:01,869 : INFO : effective_min_count=40 retains 8160 unique words (11% of original 74065, drops 65905)\n",
      "2019-04-09 20:25:01,871 : INFO : effective_min_count=40 leaves 2627273 word corpus (87% of original 2988089, drops 360816)\n",
      "2019-04-09 20:25:01,910 : INFO : deleting the raw counts dictionary of 74065 items\n",
      "2019-04-09 20:25:01,912 : INFO : sample=0.001 downsamples 30 most-common words\n",
      "2019-04-09 20:25:01,915 : INFO : downsampling leaves estimated 2494384 word corpus (94.9% of prior 2627273)\n",
      "2019-04-09 20:25:01,942 : INFO : estimated required memory for 8160 words and 300 dimensions: 23664000 bytes\n",
      "2019-04-09 20:25:01,942 : INFO : resetting layer weights\n",
      "2019-04-09 20:25:02,195 : INFO : training model with 4 workers on 8160 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=15\n",
      "2019-04-09 20:25:03,272 : INFO : EPOCH 1 - PROGRESS: at 12.22% examples, 288053 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-09 20:25:04,287 : INFO : EPOCH 1 - PROGRESS: at 25.60% examples, 310583 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-09 20:25:05,351 : INFO : EPOCH 1 - PROGRESS: at 39.12% examples, 313428 words/s, in_qsize 8, out_qsize 1\n",
      "2019-04-09 20:25:06,379 : INFO : EPOCH 1 - PROGRESS: at 52.93% examples, 319558 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-09 20:25:07,399 : INFO : EPOCH 1 - PROGRESS: at 66.71% examples, 322198 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-09 20:25:08,431 : INFO : EPOCH 1 - PROGRESS: at 79.40% examples, 319438 words/s, in_qsize 8, out_qsize 1\n",
      "2019-04-09 20:25:09,451 : INFO : EPOCH 1 - PROGRESS: at 94.14% examples, 324746 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-09 20:25:09,822 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-09 20:25:09,834 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-09 20:25:09,836 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-09 20:25:09,844 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-09 20:25:09,847 : INFO : EPOCH - 1 : training on 2988089 raw words (2494143 effective words) took 7.6s, 326683 effective words/s\n",
      "2019-04-09 20:25:10,869 : INFO : EPOCH 2 - PROGRESS: at 12.86% examples, 319456 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-09 20:25:11,884 : INFO : EPOCH 2 - PROGRESS: at 27.18% examples, 338672 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-09 20:25:12,892 : INFO : EPOCH 2 - PROGRESS: at 39.85% examples, 330196 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-09 20:25:13,899 : INFO : EPOCH 2 - PROGRESS: at 54.91% examples, 342266 words/s, in_qsize 8, out_qsize 1\n",
      "2019-04-09 20:25:14,929 : INFO : EPOCH 2 - PROGRESS: at 68.38% examples, 338244 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-09 20:25:15,957 : INFO : EPOCH 2 - PROGRESS: at 82.67% examples, 339674 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-09 20:25:16,960 : INFO : EPOCH 2 - PROGRESS: at 96.42% examples, 339334 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-09 20:25:17,121 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-09 20:25:17,165 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-09 20:25:17,172 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-09 20:25:17,204 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-09 20:25:17,206 : INFO : EPOCH - 2 : training on 2988089 raw words (2494731 effective words) took 7.3s, 339678 effective words/s\n",
      "2019-04-09 20:25:18,255 : INFO : EPOCH 3 - PROGRESS: at 12.84% examples, 310305 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-09 20:25:19,289 : INFO : EPOCH 3 - PROGRESS: at 25.94% examples, 315529 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-09 20:25:20,291 : INFO : EPOCH 3 - PROGRESS: at 39.12% examples, 320345 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-09 20:25:21,326 : INFO : EPOCH 3 - PROGRESS: at 51.03% examples, 312244 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-09 20:25:22,337 : INFO : EPOCH 3 - PROGRESS: at 65.10% examples, 318567 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-09 20:25:23,372 : INFO : EPOCH 3 - PROGRESS: at 78.35% examples, 318722 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-09 20:25:24,388 : INFO : EPOCH 3 - PROGRESS: at 91.35% examples, 318427 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-09 20:25:24,998 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-09 20:25:25,019 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-09 20:25:25,032 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-09 20:25:25,061 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-09 20:25:25,063 : INFO : EPOCH - 3 : training on 2988089 raw words (2494179 effective words) took 7.8s, 318006 effective words/s\n",
      "2019-04-09 20:25:26,082 : INFO : EPOCH 4 - PROGRESS: at 12.52% examples, 310434 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-09 20:25:27,093 : INFO : EPOCH 4 - PROGRESS: at 24.98% examples, 310772 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-09 20:25:28,130 : INFO : EPOCH 4 - PROGRESS: at 37.39% examples, 308254 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-09 20:25:29,144 : INFO : EPOCH 4 - PROGRESS: at 50.35% examples, 310974 words/s, in_qsize 8, out_qsize 1\n",
      "2019-04-09 20:25:30,153 : INFO : EPOCH 4 - PROGRESS: at 64.00% examples, 315855 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-09 20:25:31,161 : INFO : EPOCH 4 - PROGRESS: at 75.99% examples, 312750 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-09 20:25:32,161 : INFO : EPOCH 4 - PROGRESS: at 89.32% examples, 315309 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-09 20:25:32,983 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-09 20:25:32,987 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-09 20:25:33,002 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-09 20:25:33,005 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-09 20:25:33,006 : INFO : EPOCH - 4 : training on 2988089 raw words (2494464 effective words) took 7.9s, 314395 effective words/s\n",
      "2019-04-09 20:25:34,044 : INFO : EPOCH 5 - PROGRESS: at 12.22% examples, 295020 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-09 20:25:35,049 : INFO : EPOCH 5 - PROGRESS: at 25.30% examples, 312258 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-09 20:25:36,076 : INFO : EPOCH 5 - PROGRESS: at 38.07% examples, 313319 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-09 20:25:37,092 : INFO : EPOCH 5 - PROGRESS: at 51.03% examples, 314317 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-09 20:25:38,101 : INFO : EPOCH 5 - PROGRESS: at 64.00% examples, 315417 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-09 20:25:39,122 : INFO : EPOCH 5 - PROGRESS: at 76.32% examples, 313303 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-09 20:25:40,132 : INFO : EPOCH 5 - PROGRESS: at 90.03% examples, 316323 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-09 20:25:40,851 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-09 20:25:40,862 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-09 20:25:40,884 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-09 20:25:40,901 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-09 20:25:40,904 : INFO : EPOCH - 5 : training on 2988089 raw words (2494451 effective words) took 7.9s, 316286 effective words/s\n",
      "2019-04-09 20:25:40,906 : INFO : training on a 14940445 raw words (12471968 effective words) took 38.7s, 322216 effective words/s\n"
     ]
    }
   ],
   "source": [
    "# word2vec 객체를 만든다.\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "                          size=num_features, min_count = min_word_count, \\\n",
    "                          window = context, sample = downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nsa18\\anaconda3\\envs\\ml_nlp\\lib\\site-packages\\ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "train_data_vecs = get_dataset(sentences, model, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습과 검증 데이터셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, eval_input, train_label, eval_label = train_test_split(train_data_vecs, y, test_size=TEST_SIZE, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 선언 및 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 랜덤 포레스트 분류기에  100개 의사 결정 트리를 사용한다.\n",
    "forest = RandomForestClassifier(n_estimators = 100) \n",
    "\n",
    "# 단어 묶음을 벡터화한 데이터와 정답 데이터를 가지고 학습을 시작한다.\n",
    "forest.fit(train_input, train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 검증 데이터셋으로 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.852200\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %f\" % forest.score(eval_input, eval_label))  # window size 10 Accuracy: 0.841000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
