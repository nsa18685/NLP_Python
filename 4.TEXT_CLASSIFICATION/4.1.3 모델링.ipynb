{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 분류과정\n",
    "\n",
    "데이터 불러오기 -> EDA(탐색적 자료 분석) -> 데이터 정제 -> 모델링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델링\n",
    "\n",
    "### <감성 분석>\n",
    "\n",
    "1. 선형 회귀 모델 (ML)\n",
    "2. 랜덤포레스트 (ML)\n",
    "3. RNN (DL)\n",
    "4. CNN (DL)\n",
    "\n",
    "ML: scikit-learn / DL: Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 선형 회귀 모델\n",
    "\n",
    ": 종속변수와 독립변수 간의 상관관계를 모델링하는 방법 -> 특정 값을 추정 (수치)\n",
    "\n",
    "    (선형 방정식으로 표현해 예측할 데이터를 분류하는 모델 -> 선형으로 분리할 수 있음 (차원과 관계없음))\n",
    "    \n",
    "    - 2차원 : 단항 회귀 모델 vs 3차원 ~ : 다항 회귀 모델\n",
    "\n",
    "\n",
    "### 1.1 로지스틱 회귀 모델\n",
    "\n",
    ": 선형 모델의 결괏값에 로지스틱 함수를 적용해 0 ~ 1 사이의 값을 갖게 해서 확률로 표현한다. -> 이항 분류 (범주형)\n",
    "\n",
    "    (1에  가까우면 정답이 1이라고 예측하고, 0에 가까운 경우 0으로 예측한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 랜덤 포레스트 분류 모델\n",
    "\n",
    ": 여러 개의 의사결정 트리의 결괏값을 평균낸 것을 결과로 사용한다.(분류, 회귀 둘다 가능)\n",
    "\n",
    "- 많은 트리를 함께 사용함으로써 정확도가 높아진다.\n",
    "\n",
    "\n",
    "### 2.1 의사결정 트리\n",
    "\n",
    ": 자료구조 중 하나인 트리 구조와 같은 형태로 이뤄진 알고리즘이다.\n",
    "\n",
    "- 각 노드는 하나의 질문이 된다. (스무고개와 비슷)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 순환신경망 (RNN)\n",
    "\n",
    ": 현재 정보(input state)는 이전 정보(hidden stats)가 점층적으로 쌓이면서 정보를 표현할 수 있는 모델 (시퀀스에 대한 문제에 활용)\n",
    "\n",
    "- 문장 데이터를 입력해서 문장 흐름에서 패턴을 찾아 분류하게 한다. (텍스트 정보를 입력하여 특징 정보를 추출한다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 컨볼루션 신경망 분류 모델 (CNN)\n",
    "\n",
    ": 전통적인 신경망 앞에 여러 계층의 합성곱 계층을 쌓은 모델인데, 입력받은 이미지에 대한 가장 좋은 특징을 만들어 내도록 학습하고, 추출된 특징을 활용해 이미지를 분류하는 방식\n",
    "\n",
    "- (텍스트에 적용) RNN이 단어의 입력 순서를 중요하게 반영한다면, CNN은 문장의 지역 정보를 보존하면서 각 문장 성분의 등장 정보를 학습에 반영하는 구조\n",
    "\n",
    "- 각 필터 크기를 조절하면서 언어의 특징 값을 추출하게 되는데, n-gram 방식과 유사하다고 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 궁금증!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://jalammar.github.io/illustrated-word2vec/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. train으로 한 전처리 단어 인코딩을 test로 사용할 때 -> 없는 단어로 취급 or 임베딩 시, max값을 정하고 out of vocabulary로 테그 달 수 있음\n",
    "2. 단어의 bag_of_word가 매우 김 -> 잘 사용 안하기도\n",
    "3. word2vec하는 임베딩을 하면 인코딩한 단어를 연결하는 것을 보통 의미\n",
    "4. tf-idf를 만들 때, word 단위? I, love, you / char 단위? I, l,o,v,e,y,o,u + n-gram\n",
    "5. word2vec에서 window_size를 정하는 방법(vs n-gram과 비교해보기)\n",
    "6. 중앙값으로 MAX_SEQUENCE_LENGTH를 사용하는 이유? -> 미리 탐색시 확인한 값을 feature로 선정\n",
    "7. 다항회귀 vs 소프트맥스함수 -> 각각의 확률(여러개를 선택할 수 있음) vs 전체 중에 확률 (합 = 1, 동시선택 불가)\n",
    "8. cosine similarity: 두 벡터간의 유사도를 구함(벡터: 방향과 길이값을 가짐), 코사인 각도: 0˚ = 1, 90˚ = 0 (높을수록 비슷하다) -> 속도가 빠름\n",
    "9. language modeling(ex, 문자작성할 때 단어 추천, 소설쓰는 AI): 예측(가지고 있는 단어 중에, self-supervised (need pre-train)), 추천 ≒ ngram, word2vec(skip-gram, CBow)\n",
    "10. language modeling(_ _ _ -> _ ) vs skip gram(_ <- _ -> _ -> _ : 중심단어로 주변단어 예측)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
